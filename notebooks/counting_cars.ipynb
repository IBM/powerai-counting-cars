{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 IBM All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognize and Count Objects in a Video\n",
    "\n",
    "An object detection classifier can be used to identify and locate objects in a static image. When using video, you can use the same approach to static detection individual frames.  In this Jupyter Notebook, we'll use the [IBM PowerAI Vision](https://www.ibm.com/us-en/marketplace/ibm-powerai-vision) for object detection and [OpenCV Python API](https://opencv.org/) to process the video.\n",
    "\n",
    "Before running this notebook, you will need to train and deploy an object detection model. PowerAI Vision has auto-labeling to enhance your dataset for accuracy when using video input. After you train and deploy your model, set the `POWER_AI_VISION_API_URL` constant below to use your model for inference.\n",
    "\n",
    "Extracting frames and locating objects is easy with OpenCV and PowerAI Vision. The challenge is how to keep track of objects if you want to count them. As an object moves, you will need to be able to determine whether or not you have already counted the object. In this notebook, we'll use the OpenCV Tracking API to follow cars down the road while we run PowerAI Vision object detection on a sample of the frames. With tracking, we'll be able to avoid double counting without requiring a lot of code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| First Detected... | Followed Down the Road |\n",
    "| :---: | :---: |\n",
    "| ![detected](https://raw.githubusercontent.com/IBM/powerai-counting-crates/master/doc/source/images/output-frame_00011.jpg) | ![tracked](https://raw.githubusercontent.com/IBM/powerai-counting-crates/master/doc/source/images/output-frame_00128.jpg) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First setup some parameters\n",
    "\n",
    "### Required setup!\n",
    "\n",
    "Your PowerAI Vision API URL for the model that you trained and deployed will need to be set here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this URL using your PowerAI Vision host + /AIVision/api + your deployed web API URL.\n",
    "POWER_AI_VISION_API_URL = \"https://ny1.ptopenlab.com/AIVision/api/dlapis/your-guid-here\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional configuration\n",
    "\n",
    "Here you can customize some settings to tune your results.\n",
    "\n",
    "> NOTE: The notebook uses sampling and cached results to speed things up for iterative development. If you change the video, you will need to run with `CLEAN = True` to delete and regenerate your cached frames and inference results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN = False  # USE WITH CARE! Wipe out saved files when this is true (else reuse for speed)\n",
    "INPUT_VIDEO = \"assets/CarsPart3.mp4\" # The input video\n",
    "FRAMES_DIR = \"frames\"  # Output dir to hold/cache the original frames\n",
    "OUTPUT_DIR = \"output\"  # Output dir to hold the annotated frames\n",
    "SAMPLING = 10  # Classify every n frames (use tracking in between)\n",
    "CONFIDENCE = 0.80  # Confidence threshold to filter iffy objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Python Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python==3.4.0\n",
    "!pip install opencv-contrib-python\n",
    "!pip install scipy==1.0.1\n",
    "!pip install requests==2.18.4\n",
    "!pip install pandas==0.22.0\n",
    "!pip install urllib3==1.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make this compatible with Python 2.\n",
    "# from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "from IPython.display import clear_output, Image, display\n",
    "import requests\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "print(\"Warning: Certificates not verified!\")\n",
    "\n",
    "%matplotlib notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Download the video\n",
    "We will provide the example video in box and need to add a download step here.\n",
    "```\n",
    "!wget $INPUT_VIDEO_URL\n",
    "```\n",
    "\n",
    "* TODO: revisit the constant INPUT_VIDEO\n",
    "* TODO: a more pythonic way than wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or clean the directories\n",
    "Caching the frames and output directories allows the processing to continue where it left off. This is particularly useful when using a shared system with deployment time limits. This also allows you to quickly `Run all` when tweaking Python code that does not affect the inference.\n",
    "\n",
    "If you change the input video or just want a fresh start, you should `CLEAN` or change the directory names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEAN:\n",
    "    if os.path.isdir(FRAMES_DIR):\n",
    "        shutil.rmtree(FRAMES_DIR)\n",
    "    if os.path.isdir(OUTPUT_DIR):\n",
    "        shutil.rmtree(OUTPUT_DIR)\n",
    "\n",
    "if not os.path.isdir(FRAMES_DIR):\n",
    "    os.mkdir(FRAMES_DIR)\n",
    "if not os.path.isdir(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and explode the video file into JPEGs\n",
    "Each frame is saved as an individual JPEG file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(INPUT_VIDEO):\n",
    "    video_capture = cv2.VideoCapture(INPUT_VIDEO)\n",
    "else:\n",
    "    raise Exception(\"File %s doesn't exist!\" % INPUT_VIDEO)\n",
    "\n",
    "total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(\"Frame count estimate is %d\" % total_frames)\n",
    "\n",
    "num = 0\n",
    "while video_capture.get(cv2.CAP_PROP_POS_FRAMES) < video_capture.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "    success, image = video_capture.read()\n",
    "    if success:\n",
    "        num = int(video_capture.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "        print(\"Writing frame {num} of {total_frames}\".format(\n",
    "            num=num, total_frames=total_frames), end=\"\\r\")\n",
    "        cv2.imwrite('{frames_dir}/frame_{num:05d}.jpg'.format(\n",
    "            frames_dir=FRAMES_DIR, num=num), image)\n",
    "    else:\n",
    "        # TODO: If this happens, we need to add retry code\n",
    "        raise Exception('Error writing frame_{num:05d}.jpg'.format(\n",
    "            num=int(video_capture.get(cv2.CAP_PROP_POS_FRAMES))))\n",
    "\n",
    "print(\"\\nWrote {num} frames\".format(num=num))\n",
    "\n",
    "FRAME_WIDTH = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "FRAME_HEIGHT = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(\"Frame Dimensions: %sx%s\" % (FRAME_WIDTH, FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PowerAI Vision inference wrapper\n",
    "Define a helper/wrapper to call PowerAI Vision and return the inference result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = requests.Session()\n",
    "\n",
    "\n",
    "def detect_objects(filename):\n",
    "\n",
    "    # TODO: use with to open/close the file\n",
    "    f = open(filename, 'rb')\n",
    "    # WARNING! verify=False is here to allow an untrusted cert!\n",
    "    r = s.post(POWER_AI_VISION_API_URL,\n",
    "               files={'files': (filename, f)},\n",
    "               verify=False)\n",
    "    f.close()\n",
    "    return r.status_code, json.loads(r.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the API on a single frame\n",
    "Let's look at the result of a single inference operation from the PowerAI Vision Object Detection API. We see a standard HTTP return code, and a JSON response which includes the image URL, and tuples that indicate the confidence and bounding-box coordinates of the objects that we classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc, jsonresp = detect_objects('frames/frame_00100.jpg')\n",
    "\n",
    "print(\"rc = %d\" % rc)\n",
    "print(\"jsonresp: %s\" % jsonresp)\n",
    "if 'classified' in jsonresp:\n",
    "    print(\"Got back %d objects\" % len(jsonresp['classified']))\n",
    "print(json.dumps(jsonresp, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get object detection results for sampled frames\n",
    "Since we've stored all video frames on disk (for easy reference), we can iterate over those files\n",
    "and make queries as appropriate to PowerAI Vision's API. We'll store the results in a\n",
    "`tracking_results` dictionary, organized by file name. Since we are tracking objects from frame\n",
    "to frame, we can use sampling to decide how often to check for new objects.\n",
    "\n",
    "We're also caching the results so that you can change later code and run the notebook over\n",
    "without running the same inference over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Serialize requests, storing them in a \"tracking_results\" dict\n",
    "\n",
    "try:\n",
    "    with open('frames/frame-data-newmodel.json') as existing_results:\n",
    "        tracking_results = json.load(existing_results)\n",
    "except Exception:\n",
    "    # Any fail to read existing results means we start over\n",
    "    tracking_results = {}\n",
    "\n",
    "print(\"Sampling every %sth frame\" % SAMPLING)\n",
    "i = 0\n",
    "cache_used = 0\n",
    "sampled = 0\n",
    "for filename in sorted(glob.glob('frames/frame_*.jpg')):\n",
    "    i += 1\n",
    "\n",
    "    if not i % SAMPLING == 0:  # Sample every 100\n",
    "        continue\n",
    "\n",
    "    existing_result = tracking_results.get(filename)\n",
    "    if existing_result and existing_result['result'] == 'success':\n",
    "        cache_used += 1\n",
    "    else:\n",
    "        rc, results = detect_objects(filename)\n",
    "        if rc != 200 or results['result'] != 'success':\n",
    "            print(\"ERROR rc=%d for %s\" % (rc, filename))\n",
    "            print(\"ERROR result=%s\" % results)\n",
    "        else:\n",
    "            sampled += 1\n",
    "            # Save frequently to cache partial results\n",
    "            tracking_results[filename] = results\n",
    "            with open('frames/frame-data-newmodel.json', 'w') as fp:\n",
    "                json.dump(tracking_results, fp)\n",
    "\n",
    "    print(\"Processed file {num} of {total_frames} (used cache {cache_used} times)\".format(\n",
    "        num=i, total_frames=total_frames, cache_used=cache_used), end=\"\\r\")\n",
    "\n",
    "# Finally, write all our results\n",
    "with open('frames/frame-data-newmodel.json', 'w') as fp:\n",
    "    json.dump(tracking_results, fp)\n",
    "\n",
    "print(\"\\nDone\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions for tracking and drawing labels\n",
    "Refer to the [OpenCV docs.](https://docs.opencv.org/3.4.1/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI_YMAX = int(round(FRAME_HEIGHT * 0.75))  # Bottom quarter = finish line\n",
    "DROP_YMAX = int(round(FRAME_HEIGHT * 0.95))  # Near bottom = drop it\n",
    "FILLED = -1\n",
    "\n",
    "\n",
    "def update_trackers(image):\n",
    "    boxes = []\n",
    "    color2 = (240, 218, 85)\n",
    "    yellow = (66, 244, 238)\n",
    "    color = (80, 220, 60)\n",
    "    fontface = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    fontscale = 1\n",
    "    thickness = 1\n",
    "\n",
    "    for n, pair in enumerate(trackers):\n",
    "        tracker, car = pair\n",
    "        label = str(car)  # TODO: str() once when adding\n",
    "        textsize, _baseline = cv2.getTextSize(\n",
    "            str(label), fontface, fontscale, thickness)\n",
    "        _success, bbox = tracker.update(image)\n",
    "        boxes.append(bbox)  # Return updated box list\n",
    "\n",
    "        xmin = int(bbox[0])\n",
    "        ymin = int(bbox[1])\n",
    "        xmax = int(bbox[0] + bbox[2])\n",
    "        ymax = int(bbox[1] + bbox[3])\n",
    "        xmid = int(round((xmin+xmax)/2))\n",
    "        ymid = int(round((ymin+ymax)/2))\n",
    "\n",
    "        if ymax > DROP_YMAX:\n",
    "            # Drop the tracker when near the very bottom\n",
    "            del trackers[n]\n",
    "        elif ymax > ROI_YMAX:\n",
    "            # Highlight the tracker at the finish line\n",
    "            # TODO: Work-in-progress:\n",
    "            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color2, FILLED)\n",
    "        else:\n",
    "            # Rectangle and number on the cars we are tracking\n",
    "            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 4)\n",
    "            pos = (xmid - textsize[0]//2, ymid + textsize[1]//2)\n",
    "            cv2.putText(image, label, pos, fontface, 1, yellow, 4, cv2.LINE_AA)\n",
    "\n",
    "    return boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_tracked(objects, boxes):\n",
    "    if not objects:\n",
    "        return []  # No new classified objects to search for\n",
    "    if not boxes:\n",
    "        return objects  # No existing boxes, return all objects\n",
    "\n",
    "    new_objects = []\n",
    "    for obj in objects:\n",
    "        xmin = obj.get(\"xmin\", \"\")\n",
    "        xmax = obj.get(\"xmax\", \"\")\n",
    "        ymin = obj.get(\"ymin\", \"\")\n",
    "        ymax = obj.get(\"ymax\", \"\")\n",
    "        xmid = int(round((xmin+xmax)/2))\n",
    "        ymid = int(round((ymin+ymax)/2))\n",
    "        box_range = ((xmax - xmin) + (ymax - ymin)) / 2\n",
    "        for bbox in boxes:\n",
    "            bxmin = int(bbox[0])\n",
    "            bymin = int(bbox[1])\n",
    "            bxmax = int(bbox[0] + bbox[2])\n",
    "            bymax = int(bbox[1] + bbox[3])\n",
    "            bxmid = int((bxmin + bxmax) / 2)\n",
    "            bymid = int((bymin + bymax) / 2)\n",
    "            if math.sqrt((xmid - bxmid)**2 + (ymid - bymid)**2) < box_range:\n",
    "                # found existing, so break (do not add to new_objects)\n",
    "                break\n",
    "        else:\n",
    "            new_objects.append(obj)\n",
    "\n",
    "    return new_objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a small circle in the middle-middle of each detected object in the image\n",
    "def draw_label(obj, image, cars):\n",
    "    label = \"car %s\" % cars\n",
    "    color = (80, 220, 60)\n",
    "    fontface = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    fontscale = 1\n",
    "    thickness = 1\n",
    "    textsize, _baseline = cv2.getTextSize(\n",
    "        label, fontface, fontscale, thickness)\n",
    "    xmin = obj.get(\"xmin\", \"\")\n",
    "    xmax = obj.get(\"xmax\", \"\")\n",
    "    ymin = obj.get(\"ymin\", \"\")\n",
    "    ymax = obj.get(\"ymax\", \"\")\n",
    "    xmid = int(round((xmin+xmax)/2))\n",
    "    ymid = int(round((ymin+ymax)/2))\n",
    "\n",
    "    # init tracker\n",
    "    tracker = cv2.TrackerKCF_create()\n",
    "    success = tracker.init(image, (xmin, ymin, xmax-xmin, ymax-ymin))\n",
    "    if success:\n",
    "        trackers.append((tracker, cars))\n",
    "\n",
    "    # TODO: Refactor and use the update_trackers style highlighting.\n",
    "    cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 4)\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        label,\n",
    "        (xmid - textsize[0]//2, ymin + textsize[1]),\n",
    "        fontface, 1,\n",
    "        (255, 255, 255),\n",
    "        2,\n",
    "        cv2.LINE_AA)\n",
    "    cv2.circle(\n",
    "        image, (xmid, ymid), 5, color, thickness=FILLED, lineType=8, shift=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the current count of objects in the image in the upper-left corner\n",
    "def draw_count(count, name, image):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(image,\n",
    "                name + \": \" + str(count),\n",
    "                (30, 60),\n",
    "                font,\n",
    "                1,\n",
    "                (255, 255, 255),\n",
    "                2,\n",
    "                cv2.LINE_AA)\n",
    "\n",
    "\n",
    "# Draw the running total of objects in the image in the upper-left corner\n",
    "def draw_sum(count, name, image):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(image,\n",
    "                name + \" total: \" + str(count),\n",
    "                (30, 150),\n",
    "                font,\n",
    "                1,\n",
    "                (255, 255, 255),\n",
    "                2,\n",
    "                cv2.LINE_AA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference, tracking, and annotation\n",
    "Loop through the saved frames and:\n",
    "1. Update the trackers to follow already detected objects from frame to frame.\n",
    "1. Look for new objects if we ran inference on this frame.\n",
    "    * Check for overlap with tracked objects.\n",
    "    * If no overlap, assign a sequence number and start tracking.\n",
    "1. Write an annotated image with tracked objects highlighted and numbered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cars = 0\n",
    "trackers = []\n",
    "\n",
    "with open('frames/frame-data-newmodel.json') as existing_results:\n",
    "    tracking_results = json.load(existing_results)\n",
    "\n",
    "i = 0\n",
    "for filename in sorted(glob.glob('frames/frame_*.jpg')):\n",
    "    i += 1\n",
    "    img = cv2.imread(filename)\n",
    "    boxes = update_trackers(img)\n",
    "\n",
    "    if filename in tracking_results and 'classified' in tracking_results[filename]:\n",
    "        jsonresp = tracking_results[filename]\n",
    "        for obj in not_tracked(jsonresp['classified'], boxes):\n",
    "            cars += 1\n",
    "            draw_label(obj, img, cars)  # Label and start tracking\n",
    "\n",
    "    cv2.imwrite(\"output/output-\" + filename.split('/')[1], img)\n",
    "    print(\"Processed file {num} of {total_frames}\".format(\n",
    "        num=i, total_frames=total_frames), end=\"\\r\")\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play the annotated frames in the notebook\n",
    "\n",
    "This code will play the annotated frames in a loop to demonstrate the new video.\n",
    "Running this in the notebook is usually slow. Shrinking the size helps some.\n",
    "Refer to the following section to build a real, full speed video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in sorted(glob.glob(os.path.join(os.path.abspath(OUTPUT_DIR),\n",
    "                                              'output-frame_*.jpg'))):\n",
    "    frame = cv2.imread(filename)\n",
    "    clear_output(wait=True)\n",
    "    rows, columns, _channels = frame.shape\n",
    "    frame = cv2.resize(frame, (int(columns/2), int(rows/2)))  # shrink it\n",
    "    _ret, jpg = cv2.imencode('.jpg', frame)\n",
    "    display(Image(data=jpg))\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a video from the annotated frames\n",
    "\n",
    "This command requires `ffmpeg`. It will combine the annotated\n",
    "frames to build an MP4 video which you can play at full speed\n",
    "(the notebook playback above was most likely slow).\n",
    "\n",
    "Uncomment the command to try running it from this notebook, or\n",
    "copy the output files to a system with `ffmpeg` and run the\n",
    "command there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ffmpeg -y -r 60 -f image2 -i output/output-frame_%05d.jpg -vcodec libx264 -crf 25  -pix_fmt yuvj420p annotated_video.mp4\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
